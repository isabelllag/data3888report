---
title: "DATA3888 Report: Optiver 13"
format: html
editor: visual
---

## Aim and Background

**Aim**

Financial markets exhibit a high degree of uncertainty and risk, making it a challenging environment for both individual investors and financial institutions. Optiver, as a leading global market maker, specializes in creating fair, orderly, and efficient markets in a variety of financial instruments. One critical aspect that characterizes the behavior of these financial markets is 'volatility'. Volatility refers to the degree of variation observed in the price of a financial instrument over time and is a key measure of market risk.

The aim of this project is to find the model that offers the best tradeoff among accuracy, interpretability and robustness, and to investigate the effects that input stocks have on the model performance.

**Multidisciplinarity**

This project lies at the intersection of finance and data science. From the perspective of finance, understanding and predicting volatility is key to efficient portfolio management and risk mitigation. From the data science standpoint, this task represents a complex problem of time series prediction, which is generally characterized by non-linear, non-stationary, and potentially high-dimensional data. The development of a reliable volatility prediction model would therefore require a deep understanding of both these domains.

**Motivational background**

When developing models to predict volatility, there is necessarily a tradeoff between predictive power and interpretability. Indeed, Optiver identified an issue with their capacity to encourage traders to use more complex volatility models, because traders struggled to understand and therefore implement those models. Consequently, Optiver approached DATA3888 students at the University of Sydney to grapple with this tradeoff by developing a model to predict volatility, and effectively communicating that model to traders. 

As market makers, Optiver needs to quote prices at which they are willing to buy and sell financial instruments. These prices are largely influenced by the anticipated market volatility. High volatility increases the risk of market making and requires wider spreads, while low volatility allows for tighter spreads. Thus, the ability to accurately predict volatility is directly linked to Optiver's market making profitability.

Despite its significance, volatility is inherently unobservable and needs to be estimated from available market data. Traditional models like the GARCH model and its variants have limitations due to their assumptions and rigid structure. With the advent of machine learning techniques, there is potential to improve upon these traditional models by learning complex patterns from the data and offering superior predictive performance. Therefore, the generation of these models would represent a significant advancement in the profitability of the trading industry, and for Optiver in particular.

\

## Method

### A. Data Science Perspective

### B. Interdisciplinary Perspective

## Results

### A. Models

In the project, we need to evaluate the effectiveness of six distinct models across the six clusters of the stock data. We considered different strategies to evaluate our models for each cluster. There are three metrics that  we plan to use. They are  Root Mean Square Error (RMSE), the coefficient of determination (R\^2), and Mean Absolute Error (MAE). Among these metrics, we decide to use RMSE to estimate the accuracy of our models instead of MAE and R\^2. RMSE gauges the mean square magnitude of errors, it is more sensitive to outliers. This characteristic is particularly salient in the realm of financial data, which often harbors significant outliers and where the impact of sizable errors can be considerable. Contrastingly, the Mean Absolute Error (MAE) measures the absolute magnitude of errors,  it is less sensitive to outliers compared to RMSE. Also, we are not able to justify whether the error is positive or negative when we use MAE. But it is significant for volatility  to know its trends. These shortcomings also apply to R\^2. Even though R\^2  is good at providing  an overall measure of model fit, it fails to directly reflect the magnitude or direction of prediction errors.

Besides, we also test the training time of each model. Since we can see that the RMSE boxes are similar for each model in our boxplot. Therefore shorter training time of a model could be better in the rapidly changing stock market.

That\'s the mean of the RMSE of each model in each cluster. More information could be found in boxplots in the appendix. In this table, SVM and Ridge Regression performed better. But RMSE of SVM possessed a slightly dense distribution and fewer outliers in boxplots.

|                  |             |             |             |             |             |             |
|------------------|-------------|-------------|-------------|-------------|-------------|-------------|
|                  | Cluster1    | Cluster2    | Cluster3    | Cluster4    | Cluster5    | Cluster6    |
| ARMA-GARCH       | 0.000397717 | 0.000397711 | 0.000397592 | 0.000397779 | 0.000397691 | 0.00049788  |
| SVM              | 0.000234895 | 0.000138351 | 0.000160307 | 0.000325224 | 0.000228932 | 0.00017798  |
| Ridge Regression | 0.000257502 | 0.000186632 | 1.80E-04    | 0.000314913 | 0.000248003 | 0.00026557  |
| LSTM             | 0.000307353 | 0.000529648 | 0.000259467 | 0.000631368 | 0.001059554 | 0.001005189 |
| Random Forest    | 0.000651958 | 4.27E-04    | 4.30E-04    | 7.86E-04    | 5.87E-04    | 6.07E-04    |
| HAV-RV           | 0.000788709 | 0.000546498 | 0.000570423 | 0.001038327 | 0.000775287 | 0.000700853 |

```{r}
library(readxl)
# Read excel file that contains RMSEs for each model in a cluster
final <-  read_excel("C2.xlsx")
# Transfer some MSE to RMSE
final$LINEAR<- sqrt(final$LINEAR)
final$RF <- sqrt(final$RF)
# Process the data
final1 <- gather(final)
final1 <- na.omit(final1)
# We need to do it in cluster 6 since there is a super high RMSE in it.
d = which(final1$value == max(final1$value))
final1 = final1[-d,]
# Draw the boxplot
colors <- c("#0072B2", "#E69F00", "#009E73", "#F0E442", "#D55E00", "#CC79A7")
ggplot(final1, aes(x = key, y = value, fill = key)) +
  geom_boxplot(
    notch = TRUE,
    outlier.color = "black",
    outlier.shape = 16,
    width = 0.5
  ) +
  labs(x = "Models", y = "Rmse") +
  ggtitle("Boxplot for cluster6") +
  scale_fill_manual(values = colors) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 8),
    legend.position = "none",
    panel.background = element_rect(fill = "#F5F5F5")
  )
```

Also, We test the training time for each model using the same test data.

Generally, SVM and Ridge Regression are good for the data. And if you need a shorter training time of the model, Ridge Regression could be a better choice.

### B. Deployment

## Discussion

Our group identifies three main shortcomings of our project and final product. First, our project is much more focused on the data science discipline, neglecting the implications of financial knowledge. For example, we use only one method of calculating volatility, namely the standard deviation of returns, without considering other methods like beta coefficient. This potentially leads to differences in our final evaluation of models as our models' performance may change depending on the method of volatility used. Second, our clustering of the dataset is possibly futile from hindsight. We clustered the stocks using liquidity; however, after evaluation, SVM(Support Vector Machine) ends up being the best model for the majority of the clusters. A potential reason for this is that our clustering is based on only one variable, i.e. liquidity, ignoring other characteristics of a stock. A better clustering using more variables can produce a different set of clusters, thus resulting in different model performance and final evaluation. Third, our group did not use feature selection, and we could have used selection criteria like AIC or BIC to choose the most important features. Such selection can reduce our training time, and potentially change our model performance, leading to different evaluation results.

## Conclusion

The aim of our project is to develop and communicate a model that accurately predicts stock volatility and can be interpreted clearly by traders. After initial preliminary calculations on the datasets, we organised them into six clusters and used six different models - ARMA-GARCH, SVM, Regression, HAR-RV, LSTM and Random Forest - for evaluation. Using RMSE as our main evaluation metric, we found the best models for each cluster and effectively communicated our findings using an illustrative ShinyApp. For future improvements, our group identified at least three areas:

1.  Using different methods of calculating volatility for comparison.

2.  Clustering the datasets using more features of the stocks.

3.  Feature selection of the variables used for training the models.

We believe that these future works will improve our model performance.

\

## Student Contributions

## References

## Appendix

1.  Here is the coding part for clustering stocks, stock selection, process of SVM model and production of RMSE box plots.

    ```{r, echo=FALSE}
    # Prepare work for clustering
    path <- "./data3888/Optiver/individual_book_train"
    names <- dir('./data3888/Optiver/individual_book_train')
    # Get the list of file names in the directory
    file_names <- list.files(path, pattern = ".csv", full.names = TRUE)
    n <- 1
    liquidity <- NULL
    # Loop through each file, read it into a data frame, and add it to the list
    for (file in file_names) {
      name <- names[n]
      n = n+1
      df <- read.csv(file)
      dff <- NULL
      # store stock names
      dff['name'] <- name
      # calculate liquidity for each stock 
      dff['liquidity'] <-round(sum(mean(df$bid_size1), mean(df$bid_size2),mean(df$ask_size1),mean(df$ask_size2)),0)
      liquidity <- rbind(liquidity,dff)
    }
    liquidity <- as.data.frame(liquidity)
    # load the liquidity file
    l <- read.csv('liquidity edited.csv')
    # Use kmean to cluster
    kmeans <- kmeans(l[2], centers = 5)







    samples_per_cluster <- 5
    cluster_labels <- kmeans$cluster
    # Initialize an empty list to store selected samples
    selected_samples <- NULL

    # Loop through each cluster
    for (i in 1:5) {
      # Extract samples belonging to the current cluster
      current_cluster_samples <- l[cluster_labels == i, , drop = FALSE]
      
      # Randomly select samples from the current cluster
      temp <- NULL
      temp <- current_cluster_samples[sample(nrow(current_cluster_samples), samples_per_cluster), , drop = T]
      temp['cluster'] <- i
      selected_samples <- rbind(selected_samples,temp)
    }

    # Combine the selected samples from all clusters into a single matrix
    selected_samples <- do.call(rbind, selected_samples)
    selected_samples <- as.data.frame(selected_samples)








    square <- function(x) {
      return(x^2)
    }
    #make a function for calculate RMSE
    Derek <- function(path,c) {
      # data prepare 
      s <- read.csv(path)
      s <- s %>% mutate(WAP = (bid_price1 * ask_size1 +
                                 ask_price1 * bid_size1) /
                          (bid_size1 +   ask_size1))
      data <- s %>% mutate(BidAskSpread = 
                             ask_price1 / bid_price1 - 1)
      data <- data %>%  mutate(time_bucket =
                                 ceiling(seconds_in_bucket / 30))
      lr <- c(data$WAP[1],diff(log(data$WAP)))
      lr = as.data.frame(lr)
      data <- cbind(data,lr)
      id <- unique(data$time_id)
      set.seed(c)
      selected_id = sample(id, size = 100, replace = F)
      results <- data.frame()
      # Train a model for each time id
      for (i in 1:length(selected_id)){
        k <- data.frame()
        d <- data %>% filter(time_id == selected_id[i])
        d = d[-1,]
        time <- unique(d$time_bucket)
        # Take variables we need for training 
        for (y in 1:length(time)){
          df <- NULL
          dt <- d %>% filter(time_bucket == time[y])
          df['time_id'] = selected_id[i]
          df['time_bucket'] = time[y]
          df['mean_WAP'] = mean(dt$WAP)
          df['mean_spread'] = mean(dt$BidAskSpread)
          df['voladility'] = sqrt(sum(square(dt$lr)))
          df = t(df)
          df = as.data.frame(df) 
          k <- rbind(df,k)
        }
        k <- k[order(k$time_bucket),]
        i_d = k$time_id[1]
        k = k[,-1]
        # Start to train
        train_control <- trainControl(method = "timeslice",
                                      initialWindow = 4,  
                                      horizon = 1,  
                                      fixedWindow = TRUE)
        svm_model <- train(voladility ~ ., data = k, method =
                             "svmRadial", trControl =train_control)
        # Take the RMSE
        result = data.frame(i_d, svm_model$results$RMSE[1])
        results = rbind(results,result)
        
      }
      return(results)
    }






    # Get RMSEs for each cluster and prepare data for creating Boxplots
    # Read cluster file we got from kmeans
    cluster <- read.csv('sample select.csv')
    n <- length(unique(cluster$cluster))
    df_list <- list()
    # Go through each cluster
      for (i in 1:n) {
        select <- cluster %>% filter(cluster == i)
        print(i)
        final <- data.frame()
        #Go through each stock we randomly picked from each cluster
        for (p in 1:length(select$X)) {
          path = paste('./data3888/Optiver/individual_book_train/',select$X[p], sep = "")
          
          # Get RMSE in the cluster
          rmse <- Derek(path,i)
          rmse['Name'] = select$X[p]
          final = rbind(final,rmse) 
        }
        df_list[[i]] = final
      }
      # Write them as CSV
      for (i in seq_along(df_list)) {
        write.csv(df_list[[i]], file = paste0("df", i, ".csv"), row.names = FALSE)
      }








    ```

### 

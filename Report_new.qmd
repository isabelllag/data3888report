---
title: "DATA3888 Report: Optiver 13"
format: html
editor: visual
---

## Aim and Background

**Aim**

Financial markets exhibit a high degree of uncertainty and risk, making it a challenging environment for both individual investors and financial institutions. Optiver, as a leading global market maker, specializes in creating fair, orderly, and efficient markets in a variety of financial instruments. One critical aspect that characterizes the behavior of these financial markets is 'volatility'. Volatility refers to the degree of variation observed in the price of a financial instrument over time and is a key measure of market risk.

The aim of this project is to find the model that offers the best tradeoff among accuracy, interpretability and robustness, and to investigate the effects that input stocks have on the model performance.

**Multidisciplinarity**

This project lies at the intersection of finance and data science. From the perspective of finance, understanding and predicting volatility is key to efficient portfolio management and risk mitigation. From the data science standpoint, this task represents a complex problem of time series prediction, which is generally characterized by non-linear, non-stationary, and potentially high-dimensional data. The development of a reliable volatility prediction model would therefore require a deep understanding of both these domains.

**Motivational background**

When developing models to predict volatility, there is necessarily a tradeoff between predictive power and interpretability. Indeed, Optiver identified an issue with their capacity to encourage traders to use more complex volatility models, because traders struggled to understand and therefore implement those models. Consequently, Optiver approached DATA3888 students at the University of Sydney to grapple with this tradeoff by developing a model to predict volatility, and effectively communicating that model to traders. 

As market makers, Optiver needs to quote prices at which they are willing to buy and sell financial instruments. These prices are largely influenced by the anticipated market volatility. High volatility increases the risk of market making and requires wider spreads, while low volatility allows for tighter spreads. Thus, the ability to accurately predict volatility is directly linked to Optiver's market making profitability.

Despite its significance, volatility is inherently unobservable and needs to be estimated from available market data. Traditional models like the GARCH model and its variants have limitations due to their assumptions and rigid structure. With the advent of machine learning techniques, there is potential to improve upon these traditional models by learning complex patterns from the data and offering superior predictive performance. Therefore, the generation of these models would represent a significant advancement in the profitability of the trading industry, and for Optiver in particular.

\

## Method

### A. Data Science Perspective

The data we used for this project is the relevant stock data provided by Optiver. Stock data mainly consists of price data and time data. Price data include bid price, ask price, bid size and ask size. Time data contains time id and seconds in each time id. 

Once the model has been trained, it needs to be evaluated to determine how well it is performing. This can be done using a combination of graphical and quantitative metrics.

Graphical: The plots we used for evaluation include actual vs. predicted linear plots, box plots of RMSE for each cluster, QLIKE box plots for each cluster, and ACF plots. These graphics can provide a visual indication of how well the model is fitting the data.

Quantitative: Quantitative metrics provide a more objective assessment of the model's performance. For a regression task like predicting volatility, these might include metrics like Root Mean Squared Error (RMSE), and R-squared. 

We developed six models in this project: ARMA-GARCH, HAV-RV, SVM, Ridge Regression, Random Forest and LSTM.

**ARMA-GARCH**

ARMA-GARCH models are a combination of Autoregressive Moving Average (ARMA) and Generalised Autoregressive Conditional Heteroskedasticity (GARCH) models. ARMA models consist of an autoregressive element, which predicts future values based on past values, and a moving average element, which takes into account the errors from previous predictions. GARCH models predict the volatility of a time series. 

This model was chosen for this project because it is conventionally used by the finance industry to predict volatility. GARCH models with non-normal distributions are robust for predicting volatility (Liu and Morley).

The main assumption for the model is stationarity (see appendix). 

**HAV-RV**

**SVM**

SVM is a popular machine learning algorithm. Since SVM models are good at handling high-dimension data, we thought it is suitable for stock data which includes many variables for training. It is important to choose a suitable kernel in a SVM model, and  we chose Radial Basis Function (RBF) since it is good for handling non-linear and complex relationships such as volatility and other variables. RBF may overfit, however, and cost much time for training. In the training process, we circularly trained 4 time intervals then tested 1 time interval. Then we calculated the RMSE for each time ID. 

**Ridge Regression**

Ridge regression is a regularized linear regression method, which is part of the elastic network of the generalized linear regression model and controls the complexity of the model by introducing an L2 regularization term. Unlike ordinary linear regression, ridge regression adds a regularization term to the objective function, which is the product of the sum of the squares of the coefficients and a regularization parameter lambda. As a model in Elastic Net for Generalized Linear Regression, it can tune parameters. The parameter lambda determines the strength of model regularization, and the larger lambda determines the stronger regularization. When performing model tuning, different lambda values are usually tried, and the best lambda value is selected through methods such as cross-validation. This is to find a balance between bias and variance to obtain a model with better generalization ability.

**LSTM**

The LSTM (Long Short-Term Memory) deep learning model is a type of recurrent neural network designed to capture long-term dependencies in sequential data. LSTMs have memory cells controlled by input, forget, and output gates. They are chosen for research when long-term dependencies are important. LSTMs have achieved excellent results in various domains such as natural language processing and time series forecasting. For the implementation, R needs us to define the layers and corresponding parameters in the model, choosing the fitness loss function to the model, then training the model on the training data, using the validation data to adjust the prediction value. After all the epochs (like the iterative times), the model is trained and ready to use.

**Random Forest**

A random forest model works by creating many decision trees, each of which is trained on a different subset of the data. The randomness comes from both the subsets of data chosen and the subsets of features considered when splitting each node in the tree. When making a prediction, the Random Forest feeds the input data to each of the decision trees in the forest. Each tree gives a prediction independently from the others. 

The reasons why the Random Forest model was chosen for this project are robustness and flexibility. Random Forests are less likely to overfit the data compared to individual decision trees because they average the results of many different trees. Random Forest can handle both linear and non-linear relationships between predictors and the target variable, which is useful because the relationship between various factors and market volatility might not always be linear.

### B.  **Relevance to Industry Partner**

Predicting volatility is relevant to Optiver because volatility is an important element in the Black-Scholes equation, which is used for options pricing, as well as for understanding risk. 

ARMA-GARCH, HAR-RV and regression are conventionally used in the trading industry to predict volatility (cite). It was therefore relevant to Optiver to build and evaluate these models to either affirm or challenge their use by the firm, and to communicate these common models to traders. SVM, Random Forest, and LSTM are less commonly used; therefore, evaluating them and communicating them to traders was relevant to Optiver because there was scope to innovate upon the conventional models used in finance. 

Comparing a variety of models was relevant to Optiver because not only did it mean developing and explaining models to traders, but justifying them on the basis of their accuracy and time cost. The value of this method is that it enables traders to feel more confident in accepting the model as the optimal to use. Time was a particularly important evaluative element because traders must make decisions quickly in real-time; models with long runtimes, regardless of their accuracy, would be impractical for most trading environments. 

## Results

### A. Models

In the project, we need to evaluate the effectiveness of six distinct models across the six clusters of the stock data. We considered different strategies to evaluate our models for each cluster. There are three metrics that  we plan to use. They are  Root Mean Square Error (RMSE), the coefficient of determination (R\^2), and Mean Absolute Error (MAE). Among these metrics, we decide to use RMSE to estimate the accuracy of our models instead of MAE and R\^2. RMSE gauges the mean square magnitude of errors, it is more sensitive to outliers. This characteristic is particularly salient in the realm of financial data, which often harbors significant outliers and where the impact of sizable errors can be considerable. Contrastingly, the Mean Absolute Error (MAE) measures the absolute magnitude of errors,  it is less sensitive to outliers compared to RMSE. Also, we are not able to justify whether the error is positive or negative when we use MAE. But it is significant for volatility  to know its trends. These shortcomings also apply to R\^2. Even though R\^2  is good at providing  an overall measure of model fit, it fails to directly reflect the magnitude or direction of prediction errors.

Besides, we also test the training time of each model. Since we can see that the RMSE boxes are similar for each model in our boxplot. Therefore shorter training time of a model could be better in the rapidly changing stock market.

That's the mean of the RMSE of each model in each cluster. More information could be found in boxplots in the appendix. In this table, SVM and Ridge Regression performed better. But RMSE of SVM possessed a slightly dense distribution and fewer outliers in boxplots.

|                  |             |             |             |             |             |             |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
|                  | Cluster1    | Cluster2    | Cluster3    | Cluster4    | Cluster5    | Cluster6    |
| ARMA-GARCH       | 0.000397717 | 0.000397711 | 0.000397592 | 0.000397779 | 0.000397691 | 0.00049788  |
| SVM              | 0.000234895 | 0.000138351 | 0.000160307 | 0.000325224 | 0.000228932 | 0.00017798  |
| Ridge Regression | 0.000257502 | 0.000186632 | 1.80E-04    | 0.000314913 | 0.000248003 | 0.00026557  |
| LSTM             | 0.000307353 | 0.000529648 | 0.000259467 | 0.000631368 | 0.001059554 | 0.001005189 |
| Random Forest    | 0.000651958 | 4.27E-04    | 4.30E-04    | 7.86E-04    | 5.87E-04    | 6.07E-04    |
| HAV-RV           | 0.000788709 | 0.000546498 | 0.000570423 | 0.001038327 | 0.000775287 | 0.000700853 |

```{r}
library(readxl)
# Read excel file that contains RMSEs for each model in a cluster
final <-  read_excel("./dataset/result/C2.xlsx")
# Transfer some MSE to RMSE
final$LINEAR<- sqrt(final$LINEAR)
final$RF <- sqrt(final$RF)
# Process the data
final1 <- gather(final)
final1 <- na.omit(final1)
# We need to do it in cluster 6 since there is a super high RMSE in it.
d = which(final1$value == max(final1$value))
final1 = final1[-d,]
# Draw the boxplot
colors <- c("#0072B2", "#E69F00", "#009E73", "#F0E442", "#D55E00", "#CC79A7")
ggplot(final1, aes(x = key, y = value, fill = key)) +
  geom_boxplot(
    notch = TRUE,
    outlier.color = "black",
    outlier.shape = 16,
    width = 0.5
  ) +
  labs(x = "Models", y = "Rmse") +
  ggtitle("Boxplot for cluster6") +
  scale_fill_manual(values = colors) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 8),
    legend.position = "none",
    panel.background = element_rect(fill = "#F5F5F5")
  )
```

Also, We test the training time for each model using the same test data.

Generally, SVM and Ridge Regression are good for the data. And if you need a shorter training time of the model, Ridge Regression could be a better choice.

### B. Deployment

Before analyzing each stock data provided by Optiver, we need to separate them into different clusters. For data analysis, clusters can compare the differences between data sets to facilitate the subsequent search for predictive models suitable for each set of data. Since the data set is stock data, we want to classify based on the characteristics of the stock data itself. We decided to classify the liquidity of each stock data. Because liquidity is one of the important metrics in stock trading.\[1\] It reflects the trading activity of a stock market. A liquid market has a faster conversion of assets into funds. Liquidity is very important to investors. This can often reflect the time, money and cost invested by investors in the stock market.\[2\]

liquidity= sum(mean(bid_size1),mean(bid_size2),mean(ask_size1),mean(ask_size2))

All data sets can be clustered by liquidity. The K-mean is used as the clustering method. Separate the dataset into six groups, including a set of outliers. Each group consists of 5 stock data sets, a total of 30 data sets. We process and predict the economic characteristics of the 30 datasets. There are many kinds of data in each data set. We need to calculate WAP, Bidaskapread, number of orders, oversize and volatility. (insert formula).

WAP=(bid_price1 \* ask_size1 + ask_price1 \* bid_size1)/(bid_size1 + ask_size1))

BidAskSpread = ask_price / bid_price-1

num_order = bid_size1 + ask_size1 + bid_size2 + ask_size2

over_bid = sum(bid_size1) +sum(bid_size2)-sum(ask_size1)-sum(ask_size2)

volatility= √(〖sum(log⁡return)〗\^2 )

Since each time id is not sequential, the seconds data in the time id are sequential. We select 100 time ids and divide the second data in a time id into time buckets and then calculate the volatility in the time buckets predict. We use set.seed() to ensure that the time id extracted by each group member is the same. A time id is roughly 600 seconds, and we choose 30 seconds as a time bucket. Each time id is separated as training and testing data sets. The training data set is used for training the predictive model and testing data sets are used for testing the accuracy. The volatility is predicted after averaging the data in it. Each person's forecasting model predicts and calculate RMSE based on their data. The RMSE of each model in the cluster is 500. Measure the accuracy of each model in each cluster.

RMSE= √(〖mean((predicted -actual)〗\^2)) 

 

To demonstrate our work, we use the R shiny app and its interactive features. In the R shiny app, we display the RMSE boxplots of the 6 models for each cluster. And then draw a prediction diagram of the model with the highest prediction accuracy in this cluster. At the same time, R shiny app with interactive features. Users can upload a dataset they want to predict. The R shiny app will detect the cluster of the uploaded data set and select the model with the highest accuracy of the cluster to predict the volatility and return the forecast image. The main function of the R shiny app is to demonstrate the capabilities of our predictive models to customers and make customers feel involved in customizing personalized forecast results.\[3\]

## Discussion

Our group identifies three main shortcomings of our project and final product. First, our project is much more focused on the data science discipline, neglecting the implications of financial knowledge. For example, we use only one method of calculating volatility, namely the standard deviation of returns, without considering other methods like beta coefficient. This potentially leads to differences in our final evaluation of models as our models' performance may change depending on the method of volatility used. Second, our clustering of the dataset is possibly futile from hindsight. We clustered the stocks using liquidity; however, after evaluation, SVM(Support Vector Machine) ends up being the best model for the majority of the clusters. A potential reason for this is that our clustering is based on only one variable, i.e. liquidity, ignoring other characteristics of a stock. A better clustering using more variables can produce a different set of clusters, thus resulting in different model performance and final evaluation. Third, our group did not use feature selection, and we could have used selection criteria like AIC or BIC to choose the most important features. Such selection can reduce our training time, and potentially change our model performance, leading to different evaluation results.

## Conclusion

The aim of our project is to develop and communicate a model that accurately predicts stock volatility and can be interpreted clearly by traders. After initial preliminary calculations on the datasets, we organised them into six clusters and used six different models - ARMA-GARCH, SVM, Regression, HAR-RV, LSTM and Random Forest - for evaluation. Using RMSE as our main evaluation metric, we found the best models for each cluster and effectively communicated our findings using an illustrative ShinyApp. For future improvements, our group identified at least three areas:

1.  Using different methods of calculating volatility for comparison.

2.  Clustering the datasets using more features of the stocks.

3.  Feature selection of the variables used for training the models.

We believe that these future works will improve our model performance.

## Student Contributions

## References

\[1\] Ben R. Marshall, Martin Young Liquidity and stock returns in pure order-driven markets: evidence from the Australian stock market (2003) https://doi.org/10.1016/S1057-5219(03)00006-1

\[2\] ALLAUDEEN HAMEED, WENJIN KANG, S. VISWANATHAN Stock Market Declines and Liquidity (2010) https://doi.org/10.1111/j.1540-6261.2009.01529.x

\[3\] Hong Zhang, Yaobin Lu, Sumeet Gupta, Ling Zhao What motivates customers to participate in social commerce? The impact of technological environments and virtual customer experiences (2014) https://doi.org/10.1016/j.im.2014.07.005

Wei Liu and Bruce Morley, 'Volatility in the Hang Seng Index using the GARCH Approach', Asia-Pacific Financial Markets 16:51-63 (2009) https://doi.org/10.1007/s10690-009-9086-4

## Appendix

1.  Here is the coding part for clustering stocks, stock selection, process of SVM model and production of RMSE box plots.

    ```{r, echo=FALSE}
    # Prepare work for clustering
    path <- "./data3888/Optiver/individual_book_train"
    names <- dir('./data3888/Optiver/individual_book_train')
    # Get the list of file names in the directory
    file_names <- list.files(path, pattern = ".csv", full.names = TRUE)
    n <- 1
    liquidity <- NULL
    # Loop through each file, read it into a data frame, and add it to the list
    for (file in file_names) {
      name <- names[n]
      n = n+1
      df <- read.csv(file)
      dff <- NULL
      # store stock names
      dff['name'] <- name
      # calculate liquidity for each stock 
      dff['liquidity'] <-round(sum(mean(df$bid_size1), mean(df$bid_size2),mean(df$ask_size1),mean(df$ask_size2)),0)
      liquidity <- rbind(liquidity,dff)
    }
    liquidity <- as.data.frame(liquidity)
    # load the liquidity file
    l <- read.csv('liquidity edited.csv')
    # Use kmean to cluster
    kmeans <- kmeans(l[2], centers = 5)







    samples_per_cluster <- 5
    cluster_labels <- kmeans$cluster
    # Initialize an empty list to store selected samples
    selected_samples <- NULL

    # Loop through each cluster
    for (i in 1:5) {
      # Extract samples belonging to the current cluster
      current_cluster_samples <- l[cluster_labels == i, , drop = FALSE]
      
      # Randomly select samples from the current cluster
      temp <- NULL
      temp <- current_cluster_samples[sample(nrow(current_cluster_samples), samples_per_cluster), , drop = T]
      temp['cluster'] <- i
      selected_samples <- rbind(selected_samples,temp)
    }

    # Combine the selected samples from all clusters into a single matrix
    selected_samples <- do.call(rbind, selected_samples)
    selected_samples <- as.data.frame(selected_samples)








    square <- function(x) {
      return(x^2)
    }
    #make a function for calculate RMSE
    Derek <- function(path,c) {
      # data prepare 
      s <- read.csv(path)
      s <- s %>% mutate(WAP = (bid_price1 * ask_size1 +
                                 ask_price1 * bid_size1) /
                          (bid_size1 +   ask_size1))
      data <- s %>% mutate(BidAskSpread = 
                             ask_price1 / bid_price1 - 1)
      data <- data %>%  mutate(time_bucket =
                                 ceiling(seconds_in_bucket / 30))
      lr <- c(data$WAP[1],diff(log(data$WAP)))
      lr = as.data.frame(lr)
      data <- cbind(data,lr)
      id <- unique(data$time_id)
      set.seed(c)
      selected_id = sample(id, size = 100, replace = F)
      results <- data.frame()
      # Train a model for each time id
      for (i in 1:length(selected_id)){
        k <- data.frame()
        d <- data %>% filter(time_id == selected_id[i])
        d = d[-1,]
        time <- unique(d$time_bucket)
        # Take variables we need for training 
        for (y in 1:length(time)){
          df <- NULL
          dt <- d %>% filter(time_bucket == time[y])
          df['time_id'] = selected_id[i]
          df['time_bucket'] = time[y]
          df['mean_WAP'] = mean(dt$WAP)
          df['mean_spread'] = mean(dt$BidAskSpread)
          df['voladility'] = sqrt(sum(square(dt$lr)))
          df = t(df)
          df = as.data.frame(df) 
          k <- rbind(df,k)
        }
        k <- k[order(k$time_bucket),]
        i_d = k$time_id[1]
        k = k[,-1]
        # Start to train
        train_control <- trainControl(method = "timeslice",
                                      initialWindow = 4,  
                                      horizon = 1,  
                                      fixedWindow = TRUE)
        svm_model <- train(voladility ~ ., data = k, method =
                             "svmRadial", trControl =train_control)
        # Take the RMSE
        result = data.frame(i_d, svm_model$results$RMSE[1])
        results = rbind(results,result)
        
      }
      return(results)
    }






    # Get RMSEs for each cluster and prepare data for creating Boxplots
    # Read cluster file we got from kmeans
    cluster <- read.csv('sample select.csv')
    n <- length(unique(cluster$cluster))
    df_list <- list()
    # Go through each cluster
      for (i in 1:n) {
        select <- cluster %>% filter(cluster == i)
        print(i)
        final <- data.frame()
        #Go through each stock we randomly picked from each cluster
        for (p in 1:length(select$X)) {
          path = paste('./data3888/Optiver/individual_book_train/',select$X[p], sep = "")
          
          # Get RMSE in the cluster
          rmse <- Derek(path,i)
          rmse['Name'] = select$X[p]
          final = rbind(final,rmse) 
        }
        df_list[[i]] = final
      }
      # Write them as CSV
      for (i in seq_along(df_list)) {
        write.csv(df_list[[i]], file = paste0("df", i, ".csv"), row.names = FALSE)
      }








    ```

### 
